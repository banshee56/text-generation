# Text Generation

Trained models to learn temporal dependencies in text. The goal is to learn to predict the next character given a sequence of past characters. In particular, two types of recurrent neural networks are used here for text generation: a basic recurrent network utilizing a hyperbolic tangent activation function, and a long short term memory (LSTM) network.
